Text summarization is one of the many applications of natural language processing. There are two types of summarization models available. One is the extractive text summarization and the other one is the abstractive text summarization. In extractive text summarization, after the stop words are removed from the text, the frequency of occurrence of each word token is calculated. Each token is assigned a rank or weight based on frequency of occurrence, higher the frequency, greater the weight. Next, each sentence is assigned a weight by summing over the individual weights of each token present in it. The sentences are ranked per the weight and the k topmost ranking sentences are presented as the summary. While a model like this does not need any training, but this is rule based. In my personal experience, longer sentences are often selected and many a times it fails to capture the context of the text. Therefore, this article is about abstractive text summarization, which is a supervised learning model built using a transformer. A transformer uses self-attention mechanism as it’s basis. If you are not aware of attention mechanism, I have got the basics covered in my Youtube video here. The abstractive text summarization falls under sequence to sequence learning problem, where, a variable length input sequence(text in this case) is fed into the network to output another variable length sequence (summary as in here). Therefore, to deal with this type of model, we need an encoder-decoder architecture, where the encoder encodes the variable length input sequence to a fixed length encoding and feeds to the decoder. From the fixed length encoded input sequence, the decoder outputs a variable length sequence. There are several ways in which this encoder-decoder can be modelled. For sequential data processing i.e. for text data, the model can be built using RNNs only, or adding attention mechanism like Bahdanau Attention model to capture the context information better. But in case of transformers, the use of RNNs are completely bypassed. In transformers, where self-attention mechanism is used, same token is used as the query, key and value of the attention model and with some further processing, the output is calculated. Self-attention provides a major advantage of parallel processing over RNN which is strictly sequential. Suppose there is a sequence [x1 x2 x3 x4 x5] , in RNN, the tokens are sequentially processed. Like, at any instance of time the probability of generating x3 depends on the probability of generation of x2 which again, is dependent on the probability of generation of x1. Though an indirect relationship does exist between probability of generation of x3 given x1, we cannot directly predict the probability of generating token x3 two positions from x1 disregarding x2. But in self-attention technique, each token in each iteration is considered as the query and the relationship with every other tokens present in the sequence which are considered as keys are parallelly computed. Self-attention is a special case of multi head attention mechanism that assigns weights to different elements of an input sequence considering different sub spaces when generating an output sequence. It basically extends the idea of assigning weight to a token by using multiple sets of attention weights or “heads” to capture different types of relationship in the data. It helps the model decide which parts of the input are more relevant to each part of the output.